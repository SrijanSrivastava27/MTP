{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import math"
      ],
      "metadata": {
        "id": "_k_ZvcHWle7n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Half-Quadratic Splitting Module\n",
        "class HalfQuadraticSplitting(nn.Module):\n",
        "    def __init__(self, C):\n",
        "        super(HalfQuadraticSplitting, self).__init__()\n",
        "        self.filters = nn.ModuleList([nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False) for _ in range(C)])\n",
        "        self.zeta = nn.Parameter(torch.ones(C))\n",
        "        self.lambd = nn.Parameter(torch.ones(C))\n",
        "\n",
        "    def forward(self, y, k):\n",
        "        g_list, z_list = [], []\n",
        "        for i, f in enumerate(self.filters):\n",
        "            y_dft = torch.fft.fft2(y)\n",
        "            z_i_dft = torch.fft.fft2(f(y))\n",
        "            numerator = self.zeta[i] * torch.conj(z_i_dft) * y_dft\n",
        "            denominator = (self.zeta[i] * torch.abs(z_i_dft) ** 2) + 1e-5\n",
        "            g_i_dft = numerator / denominator\n",
        "            g_i = torch.fft.ifft2(g_i_dft).real\n",
        "            g_list.append(g_i)\n",
        "            # print(self.lambd[i],self.zeta[i])\n",
        "            z_list.append(F.softshrink(g_i, lambd=(self.lambd[i] * self.zeta[i]).item() ))\n",
        "\n",
        "        # Calculate k after the first end for\n",
        "        y_dft = torch.fft.fft2(y)\n",
        "        numerator = torch.sum(torch.stack([torch.conj(torch.fft.fft2(z)) * y_dft for z in z_list]), dim=0)\n",
        "        denominator = torch.sum(torch.stack([torch.abs(torch.fft.fft2(z)) ** 2 for z in z_list]), dim=0) + 1e-5\n",
        "        k = torch.fft.ifft2(numerator / denominator).real\n",
        "        k = F.relu(k)\n",
        "        k = k / k.sum()  # Normalize kernel\n",
        "\n",
        "        return g_list, z_list, k\n",
        "\n",
        "# Define the Deblurring Network (Unrolled L-Layer Network)\n",
        "class DeblurringNetwork(nn.Module):\n",
        "    def __init__(self, C, L):\n",
        "        super(DeblurringNetwork, self).__init__()\n",
        "        self.C = C\n",
        "        self.L = L\n",
        "        self.hqs_modules = nn.ModuleList([HalfQuadraticSplitting(C) for _ in range(L)])\n",
        "        self.kernels = nn.ParameterList([nn.Parameter(torch.ones(1, 1, 3, 3)) for _ in range(L)])\n",
        "\n",
        "    def forward(self, y):\n",
        "        k = torch.ones_like(y[:, :, :3, :3])  # Initial estimate for kernel (identity)\n",
        "        for l in range(self.L):\n",
        "            hqs = self.hqs_modules[l]\n",
        "            g_list, z_list, k = hqs(y, k)\n",
        "        return g_list, z_list, k"
      ],
      "metadata": {
        "id": "HZjy5WQzlo39"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_set = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "DtqlKxcIlhNw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_psnr(original, output):\n",
        "    mse = F.mse_loss(output, original)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 1.0\n",
        "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def calculate_accuracy(original, output, tolerance=0.05):\n",
        "    diff = torch.abs(original - output)\n",
        "    correct = torch.sum(diff < tolerance).item()\n",
        "    total = original.numel()\n",
        "    accuracy = correct / total\n",
        "    return accuracy * 100"
      ],
      "metadata": {
        "id": "9XbhOujkltfQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = DeblurringNetwork(C=16, L=10).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_psnr = 0\n",
        "    epoch_accuracy = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "        blurred_data_np = gaussian_filter(data.cpu().numpy(), sigma=1)\n",
        "        blurred_data = torch.tensor(blurred_data_np).to(data.device)\n",
        "\n",
        "        # Forward pass\n",
        "        g_list, z_list, output = model(blurred_data)\n",
        "\n",
        "        # Calculate loss based on the provided formula\n",
        "        loss = 0\n",
        "        for i in range(model.C):\n",
        "            f_i_y = model.hqs_modules[0].filters[i](blurred_data)\n",
        "            f_i_g = model.hqs_modules[0].filters[i](output)\n",
        "            l2_norm_term = 0.5 * torch.norm(f_i_y - f_i_g) ** 2\n",
        "            l1_norm_term = model.hqs_modules[0].lambd[i] * torch.norm(z_list[i], p=1)\n",
        "            l2_norm_diff_term = 0.5 * torch.norm(g_list[i] - z_list[i]) ** 2 / model.hqs_modules[0].zeta[i]\n",
        "            loss += l2_norm_term + l1_norm_term + l2_norm_diff_term\n",
        "        loss += 0.5 * torch.norm(output) ** 2 * 1e-5\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate PSNR for the minibatch\n",
        "        psnr = calculate_psnr(data, output)\n",
        "        epoch_psnr += psnr\n",
        "\n",
        "\n",
        "        accuracy = calculate_accuracy(data, output)\n",
        "        epoch_accuracy += accuracy\n",
        "\n",
        "        print(f\"Batch [{batch_idx+1}/{len(train_loader)}], Training PSNR: {psnr:.2f} dB, Training Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss / len(train_loader):.6f}, Training PSNR: {epoch_psnr / len(train_loader):.2f} dB, Training Accuracy: {epoch_accuracy / len(train_loader):.2f}%\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_psnr = 0\n",
        "    test_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "\n",
        "            blurred_data_np = gaussian_filter(data.cpu().numpy(), sigma=2)\n",
        "            blurred_data = torch.tensor(blurred_data_np).to(data.device)\n",
        "\n",
        "\n",
        "            g_list, z_list, output = model(blurred_data)\n",
        "\n",
        "            # Calculate loss based on the provided formula\n",
        "            loss = 0\n",
        "            for i in range(model.C):\n",
        "                f_i_y = model.hqs_modules[0].filters[i](blurred_data)\n",
        "                f_i_g = model.hqs_modules[0].filters[i](output)\n",
        "                l2_norm_term = 0.5 * torch.norm(f_i_y - f_i_g) ** 2\n",
        "                l1_norm_term = model.hqs_modules[0].lambd[i] * torch.norm(z_list[i], p=1)\n",
        "                l2_norm_diff_term = 0.5 * torch.norm(g_list[i] - z_list[i]) ** 2 / model.hqs_modules[0].zeta[i]\n",
        "                loss += l2_norm_term + l1_norm_term + l2_norm_diff_term\n",
        "            loss += 0.5 * torch.norm(output) ** 2 * 1e-5\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate PSNR for the minibatch\n",
        "            psnr = calculate_psnr(data, output)\n",
        "            test_psnr += psnr\n",
        "\n",
        "            # Calculate accuracy for the minibatch\n",
        "            accuracy = calculate_accuracy(data, output)\n",
        "            test_accuracy += accuracy\n",
        "\n",
        "            print(f\"Batch [{batch_idx+1}/{len(test_loader)}], Test PSNR: {psnr:.2f} dB, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss / len(test_loader):.6f}, Test PSNR: {test_psnr / len(test_loader):.2f} dB, Test Accuracy: {test_accuracy / len(test_loader):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TuuQj4N3lc4f",
        "outputId": "5402b077-c143-458d-cfad-c41a603e0c0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [1/938], Training PSNR: 9.22 dB, Training Accuracy: 80.62%\n",
            "Batch [2/938], Training PSNR: 9.77 dB, Training Accuracy: 82.74%\n",
            "Batch [3/938], Training PSNR: 9.92 dB, Training Accuracy: 82.95%\n",
            "Batch [4/938], Training PSNR: 9.42 dB, Training Accuracy: 81.73%\n",
            "Batch [5/938], Training PSNR: 9.30 dB, Training Accuracy: 81.20%\n",
            "Batch [6/938], Training PSNR: 9.37 dB, Training Accuracy: 81.10%\n",
            "Batch [7/938], Training PSNR: 9.66 dB, Training Accuracy: 82.19%\n",
            "Batch [8/938], Training PSNR: 9.12 dB, Training Accuracy: 80.25%\n",
            "Batch [9/938], Training PSNR: 9.32 dB, Training Accuracy: 80.94%\n",
            "Batch [10/938], Training PSNR: 9.53 dB, Training Accuracy: 81.84%\n",
            "Batch [11/938], Training PSNR: 9.33 dB, Training Accuracy: 80.95%\n",
            "Batch [12/938], Training PSNR: 9.55 dB, Training Accuracy: 81.85%\n",
            "Batch [13/938], Training PSNR: 9.75 dB, Training Accuracy: 82.42%\n",
            "Batch [14/938], Training PSNR: 9.65 dB, Training Accuracy: 82.46%\n",
            "Batch [15/938], Training PSNR: 9.36 dB, Training Accuracy: 80.94%\n",
            "Batch [16/938], Training PSNR: 9.26 dB, Training Accuracy: 80.90%\n",
            "Batch [17/938], Training PSNR: 9.41 dB, Training Accuracy: 81.39%\n",
            "Batch [18/938], Training PSNR: 9.74 dB, Training Accuracy: 82.50%\n",
            "Batch [19/938], Training PSNR: 9.63 dB, Training Accuracy: 82.16%\n",
            "Batch [20/938], Training PSNR: 9.65 dB, Training Accuracy: 82.32%\n",
            "Batch [21/938], Training PSNR: 9.34 dB, Training Accuracy: 81.04%\n",
            "Batch [22/938], Training PSNR: 9.58 dB, Training Accuracy: 81.98%\n",
            "Batch [23/938], Training PSNR: 9.29 dB, Training Accuracy: 81.04%\n",
            "Batch [24/938], Training PSNR: 9.28 dB, Training Accuracy: 80.98%\n",
            "Batch [25/938], Training PSNR: 9.55 dB, Training Accuracy: 81.84%\n",
            "Batch [26/938], Training PSNR: 9.63 dB, Training Accuracy: 82.01%\n",
            "Batch [27/938], Training PSNR: 9.68 dB, Training Accuracy: 82.49%\n",
            "Batch [28/938], Training PSNR: 9.52 dB, Training Accuracy: 81.80%\n",
            "Batch [29/938], Training PSNR: 9.38 dB, Training Accuracy: 81.52%\n",
            "Batch [30/938], Training PSNR: 9.54 dB, Training Accuracy: 81.65%\n",
            "Batch [31/938], Training PSNR: 9.20 dB, Training Accuracy: 80.85%\n",
            "Batch [32/938], Training PSNR: 9.45 dB, Training Accuracy: 81.90%\n",
            "Batch [33/938], Training PSNR: 9.23 dB, Training Accuracy: 81.05%\n",
            "Batch [34/938], Training PSNR: 9.57 dB, Training Accuracy: 81.90%\n",
            "Batch [35/938], Training PSNR: 9.30 dB, Training Accuracy: 80.96%\n",
            "Batch [36/938], Training PSNR: 9.73 dB, Training Accuracy: 82.55%\n",
            "Batch [37/938], Training PSNR: 9.70 dB, Training Accuracy: 82.46%\n",
            "Batch [38/938], Training PSNR: 9.59 dB, Training Accuracy: 81.95%\n",
            "Batch [39/938], Training PSNR: 9.54 dB, Training Accuracy: 81.65%\n",
            "Batch [40/938], Training PSNR: 9.13 dB, Training Accuracy: 80.70%\n",
            "Batch [41/938], Training PSNR: 9.35 dB, Training Accuracy: 81.37%\n",
            "Batch [42/938], Training PSNR: 9.38 dB, Training Accuracy: 81.22%\n",
            "Batch [43/938], Training PSNR: 9.19 dB, Training Accuracy: 80.67%\n",
            "Batch [44/938], Training PSNR: 9.70 dB, Training Accuracy: 82.40%\n",
            "Batch [45/938], Training PSNR: 9.23 dB, Training Accuracy: 80.72%\n",
            "Batch [46/938], Training PSNR: 9.43 dB, Training Accuracy: 81.40%\n",
            "Batch [47/938], Training PSNR: 9.56 dB, Training Accuracy: 81.62%\n",
            "Batch [48/938], Training PSNR: 9.64 dB, Training Accuracy: 81.85%\n",
            "Batch [49/938], Training PSNR: 9.56 dB, Training Accuracy: 82.22%\n",
            "Batch [50/938], Training PSNR: 9.57 dB, Training Accuracy: 81.91%\n",
            "Batch [51/938], Training PSNR: 9.31 dB, Training Accuracy: 81.09%\n",
            "Batch [52/938], Training PSNR: 9.52 dB, Training Accuracy: 82.07%\n",
            "Batch [53/938], Training PSNR: 9.40 dB, Training Accuracy: 81.49%\n",
            "Batch [54/938], Training PSNR: 9.65 dB, Training Accuracy: 82.19%\n",
            "Batch [55/938], Training PSNR: 9.42 dB, Training Accuracy: 81.71%\n",
            "Batch [56/938], Training PSNR: 9.61 dB, Training Accuracy: 82.32%\n",
            "Batch [57/938], Training PSNR: 9.64 dB, Training Accuracy: 82.50%\n",
            "Batch [58/938], Training PSNR: 9.60 dB, Training Accuracy: 81.82%\n",
            "Batch [59/938], Training PSNR: 9.81 dB, Training Accuracy: 82.50%\n",
            "Batch [60/938], Training PSNR: 9.27 dB, Training Accuracy: 81.05%\n",
            "Batch [61/938], Training PSNR: 9.43 dB, Training Accuracy: 81.31%\n",
            "Batch [62/938], Training PSNR: 9.72 dB, Training Accuracy: 82.40%\n",
            "Batch [63/938], Training PSNR: 9.50 dB, Training Accuracy: 81.57%\n",
            "Batch [64/938], Training PSNR: 9.56 dB, Training Accuracy: 82.01%\n",
            "Batch [65/938], Training PSNR: 9.31 dB, Training Accuracy: 81.21%\n",
            "Batch [66/938], Training PSNR: 9.50 dB, Training Accuracy: 81.92%\n",
            "Batch [67/938], Training PSNR: 9.43 dB, Training Accuracy: 82.03%\n",
            "Batch [68/938], Training PSNR: 9.48 dB, Training Accuracy: 81.66%\n",
            "Batch [69/938], Training PSNR: 9.61 dB, Training Accuracy: 81.95%\n",
            "Batch [70/938], Training PSNR: 9.48 dB, Training Accuracy: 81.67%\n",
            "Batch [71/938], Training PSNR: 9.74 dB, Training Accuracy: 82.27%\n",
            "Batch [72/938], Training PSNR: 9.48 dB, Training Accuracy: 81.50%\n",
            "Batch [73/938], Training PSNR: 9.12 dB, Training Accuracy: 80.45%\n",
            "Batch [74/938], Training PSNR: 9.26 dB, Training Accuracy: 80.79%\n",
            "Batch [75/938], Training PSNR: 9.64 dB, Training Accuracy: 82.32%\n",
            "Batch [76/938], Training PSNR: 9.56 dB, Training Accuracy: 82.12%\n",
            "Batch [77/938], Training PSNR: 9.65 dB, Training Accuracy: 82.18%\n",
            "Batch [78/938], Training PSNR: 9.46 dB, Training Accuracy: 81.76%\n",
            "Batch [79/938], Training PSNR: 9.48 dB, Training Accuracy: 81.76%\n",
            "Batch [80/938], Training PSNR: 9.50 dB, Training Accuracy: 81.82%\n",
            "Batch [81/938], Training PSNR: 9.54 dB, Training Accuracy: 81.58%\n",
            "Batch [82/938], Training PSNR: 9.49 dB, Training Accuracy: 81.68%\n",
            "Batch [83/938], Training PSNR: 9.50 dB, Training Accuracy: 81.62%\n",
            "Batch [84/938], Training PSNR: 9.56 dB, Training Accuracy: 81.78%\n",
            "Batch [85/938], Training PSNR: 9.59 dB, Training Accuracy: 81.80%\n",
            "Batch [86/938], Training PSNR: 9.69 dB, Training Accuracy: 82.32%\n",
            "Batch [87/938], Training PSNR: 10.20 dB, Training Accuracy: 83.58%\n",
            "Batch [88/938], Training PSNR: 9.32 dB, Training Accuracy: 81.05%\n",
            "Batch [89/938], Training PSNR: 9.62 dB, Training Accuracy: 81.98%\n",
            "Batch [90/938], Training PSNR: 9.72 dB, Training Accuracy: 82.35%\n",
            "Batch [91/938], Training PSNR: 9.56 dB, Training Accuracy: 81.74%\n",
            "Batch [92/938], Training PSNR: 9.64 dB, Training Accuracy: 82.15%\n",
            "Batch [93/938], Training PSNR: 9.49 dB, Training Accuracy: 81.95%\n",
            "Batch [94/938], Training PSNR: 9.72 dB, Training Accuracy: 82.41%\n",
            "Batch [95/938], Training PSNR: 9.21 dB, Training Accuracy: 81.10%\n",
            "Batch [96/938], Training PSNR: 9.91 dB, Training Accuracy: 82.57%\n",
            "Batch [97/938], Training PSNR: 9.51 dB, Training Accuracy: 81.72%\n",
            "Batch [98/938], Training PSNR: 9.52 dB, Training Accuracy: 81.95%\n",
            "Batch [99/938], Training PSNR: 9.42 dB, Training Accuracy: 81.22%\n",
            "Batch [100/938], Training PSNR: 9.49 dB, Training Accuracy: 81.50%\n",
            "Batch [101/938], Training PSNR: 9.82 dB, Training Accuracy: 82.33%\n",
            "Batch [102/938], Training PSNR: 9.72 dB, Training Accuracy: 82.53%\n",
            "Batch [103/938], Training PSNR: 9.68 dB, Training Accuracy: 82.13%\n",
            "Batch [104/938], Training PSNR: 9.45 dB, Training Accuracy: 81.79%\n",
            "Batch [105/938], Training PSNR: 9.55 dB, Training Accuracy: 81.99%\n",
            "Batch [106/938], Training PSNR: 9.63 dB, Training Accuracy: 82.21%\n",
            "Batch [107/938], Training PSNR: 9.66 dB, Training Accuracy: 82.14%\n",
            "Batch [108/938], Training PSNR: 9.52 dB, Training Accuracy: 81.85%\n",
            "Batch [109/938], Training PSNR: 9.73 dB, Training Accuracy: 82.50%\n",
            "Batch [110/938], Training PSNR: 9.61 dB, Training Accuracy: 82.37%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7bb4e46754a8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblurred_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate loss based on the provided formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0742aace3e03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mhqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhqs_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0742aace3e03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y, k)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i_dft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mg_i_dft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mg_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mifft2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_i_dft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# print(self.lambd[i],self.zeta[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}